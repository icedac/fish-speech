<div align="center">
<h1>Fish Speech</h1>

**English** | [ç®€ä½“ä¸­æ–‡](docs/README.zh.md) | [Portuguese](docs/README.pt-BR.md) | [æ—¥æœ¬èª](docs/README.ja.md) | [í•œêµ­ì–´](docs/README.ko.md) <br>

<a href="https://www.producthunt.com/posts/fish-speech-1-4?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-fish&#0045;speech&#0045;1&#0045;4" target="_blank">
    <img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=488440&theme=light" alt="Fish&#0032;Speech&#0032;1&#0046;4 - Open&#0045;Source&#0032;Multilingual&#0032;Text&#0045;to&#0045;Speech&#0032;with&#0032;Voice&#0032;Cloning | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" />
</a>
<a href="https://trendshift.io/repositories/7014" target="_blank">
    <img src="https://trendshift.io/api/badge/repositories/7014" alt="fishaudio%2Ffish-speech | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>
</a>
<br>
</div>
<br>

<div align="center">
    <img src="https://count.getloli.com/get/@fish-speech?theme=asoul" /><br>
</div>

<br>

<div align="center">
    <a target="_blank" href="https://discord.gg/Es5qTB9BcN">
        <img alt="Discord" src="https://img.shields.io/discord/1214047546020728892?color=%23738ADB&label=Discord&logo=discord&logoColor=white&style=flat-square"/>
    </a>
    <a target="_blank" href="https://hub.docker.com/r/fishaudio/fish-speech">
        <img alt="Docker" src="https://img.shields.io/docker/pulls/fishaudio/fish-speech?style=flat-square&logo=docker"/>
    </a>
    <a target="_blank" href="https://huggingface.co/spaces/fishaudio/fish-speech-1">
        <img alt="Huggingface" src="https://img.shields.io/badge/ğŸ¤—%20-space%20demo-yellow"/>
    </a>
    <a target="_blank" href="https://pd.qq.com/s/bwxia254o">
      <img alt="QQ Channel" src="https://img.shields.io/badge/QQ-blue?logo=tencentqq">
    </a>
</div>

This codebase is released under Apache License and all model weights are released under CC-BY-NC-SA-4.0 License. Please refer to [LICENSE](LICENSE) for more details.

---
## Fish Agent
We are very excited to announce that we have made our self-research agent demo open source, you can now try our agent demo for instant English and Chinese chat locally by following the [docs](https://speech.fish.audio/start_agent/).

You should mention that the content is released under a **CC BY-NC-SA 4.0 licence**. And the demo is an early alpha test version, the inference speed needs to be optimised, and there are a lot of bugs waiting to be fixed. If you've found a bug or want to fix it, we'd be very happy to receive an issue or a pull request.

## Features
### Fish Speech

1. **Zero-shot & Few-shot TTS:** Input a 10 to 30-second vocal sample to generate high-quality TTS output. **For detailed guidelines, see [Voice Cloning Best Practices](https://docs.fish.audio/text-to-speech/voice-clone-best-practices).**

2. **Multilingual & Cross-lingual Support:** Simply copy and paste multilingual text into the input boxâ€”no need to worry about the language. Currently supports English, Japanese, Korean, Chinese, French, German, Arabic, and Spanish.

3. **No Phoneme Dependency:** The model has strong generalization capabilities and does not rely on phonemes for TTS. It can handle text in any language script.

4. **Highly Accurate:** Achieves a low CER (Character Error Rate) and WER (Word Error Rate) of around 2% for 5-minute English texts.

5. **Fast:** With fish-tech acceleration, the real-time factor is approximately 1:5 on an Nvidia RTX 4060 laptop and 1:15 on an Nvidia RTX 4090.

6. **WebUI Inference:** Features an easy-to-use, Gradio-based web UI compatible with Chrome, Firefox, Edge, and other browsers.

7. **GUI Inference:** Offers a PyQt6 graphical interface that works seamlessly with the API server. Supports Linux, Windows, and macOS. [See GUI](https://github.com/AnyaCoder/fish-speech-gui).

8. **Deploy-Friendly:** Easily set up an inference server with native support for Linux, Windows and MacOS, minimizing speed loss.

### Fish Agent
1. **Completely End to End:** Automatically integrates ASR and TTS parts, no need to plug-in other models, i.e., true end-to-end, not three-stage (ASR+LLM+TTS).

2. **Timbre Control:** Can use reference audio to control the speech timbre.

3. **Emotional:** The model can generate speech with strong emotion.

## Disclaimer

We do not hold any responsibility for any illegal usage of the codebase. Please refer to your local laws about DMCA and other related laws.

## Online Demo

[Fish Audio](https://fish.audio)

[Fish Agent](https://fish.audio/demo/live)

## Quick Start for Local Inference 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/fishaudio/fish-speech/blob/main/inference.ipynb)

## Videos

#### V1.5 Demo Video: [Watch the video on X (Twitter).](https://x.com/FishAudio/status/1864370933496205728)

## Documents

- [English](https://speech.fish.audio/)
- [ä¸­æ–‡](https://speech.fish.audio/zh/)
- [æ—¥æœ¬èª](https://speech.fish.audio/ja/)
- [Portuguese (Brazil)](https://speech.fish.audio/pt/)

## Samples (2024/10/02 V1.4)

- [English](https://speech.fish.audio/samples/)
- [ä¸­æ–‡](https://speech.fish.audio/zh/samples/)
- [æ—¥æœ¬èª](https://speech.fish.audio/ja/samples/)
- [Portuguese (Brazil)](https://speech.fish.audio/pt/samples/)

## Credits

- [VITS2 (daniilrobnikov)](https://github.com/daniilrobnikov/vits2)
- [Bert-VITS2](https://github.com/fishaudio/Bert-VITS2)
- [GPT VITS](https://github.com/innnky/gpt-vits)
- [MQTTS](https://github.com/b04901014/MQTTS)
- [GPT Fast](https://github.com/pytorch-labs/gpt-fast)
- [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)

## Tech Report (V1.4)
```bibtex
@misc{fish-speech-v1.4,
      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},
      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},
      year={2024},
      eprint={2411.01156},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2411.01156},
}
```

## Sponsor

<div>
  <a href="https://6block.com/">
    <img src="https://avatars.githubusercontent.com/u/60573493" width="100" height="100" alt="6Block Avatar"/>
  </a>
  <br>
  <a href="https://6block.com/">Data Processing sponsor by 6Block</a>
</div>



# Prerequisites
## Miniconda
- https://www.anaconda.com/docs/getting-started/miniconda/main

## Model Downloads
```bash
pip install -U "huggingface_hub[cli]"
huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5
```

## Basic Test
```bash
python -m tools.run_webui \
    --llama-checkpoint-path "checkpoints/fish-speech-1.5" \
    --decoder-checkpoint-path "checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth" \
    --decoder-config-name firefly_gan_vq
```

# Local Testing Guide (VoiceReel Multi-Speaker TTS)

## ğŸš€ Quick Start: Two-Speaker Dialogue Test

This guide will help you test VoiceReel's multi-speaker TTS locally with a two-person dialogue example.

### 1. Environment Setup

```bash
# Create conda environment
conda create -n voicereel python=3.10
conda activate voicereel

# Install dependencies
pip install -e ".[stable]"
pip install redis celery psycopg2-binary boto3

# Install Fish-Speech dependencies
pip install torch torchaudio transformers gradio loguru

# Download Fish-Speech models
huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5
```

### 2. Start Required Services

#### Option A: Using Docker Compose (Recommended)
```bash
# Start PostgreSQL, Redis, and VoiceReel
docker-compose -f docker-compose.dev.yml up -d
```

#### Option B: Manual Setup
```bash
# Terminal 1: Start Redis
redis-server

# Terminal 2: Start PostgreSQL (if not installed, use Docker)
docker run -d --name voicereel-postgres \
    -e POSTGRES_USER=voicereel \
    -e POSTGRES_PASSWORD=voicereel \
    -e POSTGRES_DB=voicereel \
    -p 5432:5432 \
    postgres:15-alpine

# Terminal 3: Start Celery Worker
export VR_POSTGRES_DSN="postgresql://voicereel:voicereel@localhost:5432/voicereel"
export VR_REDIS_URL="redis://localhost:6379/0"
celery -A voicereel.tasks worker --loglevel=info
```

### 3. Initialize Database

```bash
# Set database URL
export VR_POSTGRES_DSN="postgresql://voicereel:voicereel@localhost:5432/voicereel"

# Run migration
python tools/migrate_to_postgres.py
```

### 4. Start VoiceReel Server

```bash
# Terminal 4: Start VoiceReel API Server
export VR_POSTGRES_DSN="postgresql://voicereel:voicereel@localhost:5432/voicereel"
export VR_REDIS_URL="redis://localhost:6379/0"
export VR_API_KEY="test-api-key-12345"
export VR_LOG_LEVEL="INFO"
export VR_DEBUG="true"

python -m voicereel.server_postgres
```

### 5. Prepare Reference Audio Files

Create two reference audio files for your speakers:

```bash
# Create test audio directory
mkdir -p test_audio

# Record or download 30-second samples for each speaker
# Speaker 1: Male voice (e.g., news anchor style)
# Save as: test_audio/speaker1_male.wav

# Speaker 2: Female voice (e.g., conversational style)  
# Save as: test_audio/speaker2_female.wav

# Create matching transcripts
echo "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ì²« ë²ˆì§¸ í™”ìì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. ì´ ìŒì„±ì€ ì œ ëª©ì†Œë¦¬ ìƒ˜í”Œì…ë‹ˆë‹¤." > test_audio/speaker1_script.txt
echo "ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ë‘ ë²ˆì§¸ í™”ìì˜ˆìš”. ë§ì•„ìš”, ì •ë§ í™”ì°½í•œ ë‚ ì”¨ë„¤ìš”. ì œ ëª©ì†Œë¦¬ë„ ì˜ ë“¤ë¦¬ì‹œë‚˜ìš”?" > test_audio/speaker2_script.txt
```

### 6. Register Speakers

```python
# test_register_speakers.py
import requests
import time

API_URL = "http://localhost:8080"
API_KEY = "test-api-key-12345"

headers = {
    "X-VR-APIKEY": API_KEY
}

# Register Speaker 1 (Male)
with open("test_audio/speaker1_male.wav", "rb") as audio_file:
    with open("test_audio/speaker1_script.txt", "r") as script_file:
        files = {
            "reference_audio": ("speaker1.wav", audio_file, "audio/wav"),
        }
        data = {
            "name": "ê¹€ë¯¼ìˆ˜ (ë‚¨ì„± ì•µì»¤)",
            "lang": "ko",
            "reference_script": script_file.read()
        }
        
        response = requests.post(
            f"{API_URL}/v1/speakers",
            headers=headers,
            files=files,
            data=data
        )
        print("Speaker 1 Registration:", response.json())
        speaker1_job = response.json()["job_id"]

# Register Speaker 2 (Female)
with open("test_audio/speaker2_female.wav", "rb") as audio_file:
    with open("test_audio/speaker2_script.txt", "r") as script_file:
        files = {
            "reference_audio": ("speaker2.wav", audio_file, "audio/wav"),
        }
        data = {
            "name": "ì´ìˆ˜ì§„ (ì—¬ì„± ì§„í–‰ì)",
            "lang": "ko",
            "reference_script": script_file.read()
        }
        
        response = requests.post(
            f"{API_URL}/v1/speakers",
            headers=headers,
            files=files,
            data=data
        )
        print("Speaker 2 Registration:", response.json())
        speaker2_job = response.json()["job_id"]

# Wait for registration to complete
print("\nWaiting for speaker registration...")
time.sleep(10)

# Check registration status
for job_id in [speaker1_job, speaker2_job]:
    response = requests.get(f"{API_URL}/v1/jobs/{job_id}", headers=headers)
    print(f"Job {job_id} status:", response.json())

# Get speaker list
response = requests.get(f"{API_URL}/v1/speakers", headers=headers)
speakers = response.json()["speakers"]
print("\nRegistered speakers:")
for speaker in speakers:
    print(f"- {speaker['name']}: {speaker['id']}")
```

### 7. Test Two-Speaker Dialogue

```python
# test_dialogue.py
import requests
import json
import time
import subprocess

API_URL = "http://localhost:8080"
API_KEY = "test-api-key-12345"

headers = {
    "X-VR-APIKEY": API_KEY,
    "Content-Type": "application/json"
}

# Get speaker IDs
response = requests.get(f"{API_URL}/v1/speakers", headers=headers)
speakers = response.json()["speakers"]
speaker1_id = speakers[0]["id"]  # ê¹€ë¯¼ìˆ˜
speaker2_id = speakers[1]["id"]  # ì´ìˆ˜ì§„

# Create dialogue script
dialogue_script = [
    {"speaker_id": speaker1_id, "text": "ì•ˆë…•í•˜ì„¸ìš”, ì‹œì²­ì ì—¬ëŸ¬ë¶„. ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë¥¼ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."},
    {"speaker_id": speaker2_id, "text": "ë„¤, ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ì „êµ­ì ìœ¼ë¡œ ë§‘ì€ ë‚ ì”¨ê°€ ì´ì–´ì§€ê² ìŠµë‹ˆë‹¤."},
    {"speaker_id": speaker1_id, "text": "ê·¸ë ‡êµ°ìš”. ë¯¸ì„¸ë¨¼ì§€ ìƒí™©ì€ ì–´ë–¤ê°€ìš”?"},
    {"speaker_id": speaker2_id, "text": "ë‹¤í–‰íˆ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” 'ì¢‹ìŒ' ìˆ˜ì¤€ì„ ìœ ì§€í•˜ê³  ìˆì–´ìš”. ì•¼ì™¸ í™œë™í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤."},
    {"speaker_id": speaker1_id, "text": "ì¢‹ì€ ì†Œì‹ì´ë„¤ìš”. ì´ë²ˆ ì£¼ë§ ë‚ ì”¨ëŠ” ì–´ë–¨ê¹Œìš”?"},
    {"speaker_id": speaker2_id, "text": "ì£¼ë§ì—ë„ í™”ì°½í•œ ë‚ ì”¨ê°€ ê³„ì†ë  ì˜ˆì •ì…ë‹ˆë‹¤. ë‚˜ë“¤ì´ ê³„íšì´ ìˆìœ¼ì‹  ë¶„ë“¤ê»˜ëŠ” í¬ì†Œì‹ì´ë„¤ìš”."},
    {"speaker_id": speaker1_id, "text": "ê°ì‚¬í•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!"},
    {"speaker_id": speaker2_id, "text": "ë„¤, ì—¬ëŸ¬ë¶„ë„ ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”!"}
]

# Request synthesis
synthesis_request = {
    "script": dialogue_script,
    "output_format": "wav",
    "sample_rate": 44100,
    "caption_format": "vtt"
}

print("Requesting dialogue synthesis...")
response = requests.post(
    f"{API_URL}/v1/synthesize",
    headers=headers,
    json=synthesis_request
)

if response.status_code == 202:
    job_id = response.json()["job_id"]
    print(f"Synthesis job created: {job_id}")
    
    # Poll for completion
    while True:
        time.sleep(3)
        response = requests.get(f"{API_URL}/v1/jobs/{job_id}", headers=headers)
        job_status = response.json()
        
        print(f"Status: {job_status['status']}")
        
        if job_status["status"] == "succeeded":
            audio_url = job_status["audio_url"]
            captions = job_status["captions"]
            
            print(f"\nâœ… Synthesis completed!")
            print(f"Audio URL: {audio_url}")
            
            # Download audio file
            audio_response = requests.get(audio_url)
            with open("dialogue_output.wav", "wb") as f:
                f.write(audio_response.content)
            print("Audio saved as: dialogue_output.wav")
            
            # Save captions
            with open("dialogue_output.vtt", "w") as f:
                f.write(captions)
            print("Captions saved as: dialogue_output.vtt")
            
            # Display dialogue timing
            print("\nDialogue Timing:")
            for caption in job_status.get("caption_data", []):
                speaker_name = "ê¹€ë¯¼ìˆ˜" if caption["speaker"] == speaker1_id else "ì´ìˆ˜ì§„"
                print(f"{caption['start']:.2f}s - {caption['end']:.2f}s [{speaker_name}]: {caption['text']}")
            
            # Play audio (macOS/Linux)
            try:
                if subprocess.run(["which", "afplay"], capture_output=True).returncode == 0:
                    subprocess.run(["afplay", "dialogue_output.wav"])
                elif subprocess.run(["which", "aplay"], capture_output=True).returncode == 0:
                    subprocess.run(["aplay", "dialogue_output.wav"])
                else:
                    print("\nğŸµ Please play 'dialogue_output.wav' with your audio player")
            except:
                print("\nğŸµ Please play 'dialogue_output.wav' with your audio player")
            
            break
            
        elif job_status["status"] == "failed":
            print(f"âŒ Synthesis failed: {job_status.get('error', 'Unknown error')}")
            break
else:
    print(f"âŒ Request failed: {response.text}")
```

### 8. Alternative: Using VoiceReel Client

```python
# test_with_client.py
from voicereel.client import VoiceReelClient
import asyncio

async def test_dialogue():
    # Initialize client
    client = VoiceReelClient(
        base_url="http://localhost:8080",
        api_key="test-api-key-12345"
    )
    
    # Register speakers (if not already registered)
    # ... (similar to above)
    
    # Get speakers
    speakers = await client.get_speakers()
    print("Available speakers:")
    for speaker in speakers:
        print(f"- {speaker['name']} ({speaker['id']})")
    
    # Create dialogue
    dialogue = [
        {"speaker_id": speakers[0]["id"], "text": "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ì…ë‹ˆë‹¤."},
        {"speaker_id": speakers[1]["id"], "text": "ë„¤, ì£¼ìš” ì†Œì‹ì„ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."},
    ]
    
    # Synthesize
    job_id = await client.synthesize(dialogue, output_format="wav")
    print(f"Synthesis job: {job_id}")
    
    # Wait for completion
    result = await client.wait_for_job(job_id)
    
    # Download audio
    await client.download_audio(result["audio_url"], "client_output.wav")
    print("âœ… Audio saved as: client_output.wav")

# Run the test
asyncio.run(test_dialogue())
```

## ğŸ” Troubleshooting

### Common Issues

1. **"Connection refused" error**
   - Check if all services are running (Redis, PostgreSQL, Celery, API server)
   - Verify ports are not in use: `lsof -i :8080,5432,6379`

2. **"Speaker registration failed"**
   - Ensure audio files are at least 30 seconds long
   - Check audio format (WAV, 16kHz+ recommended)
   - Verify transcript matches the audio content

3. **"Synthesis takes too long"**
   - Check GPU availability: `nvidia-smi`
   - Monitor Celery worker logs for errors
   - Reduce dialogue length for initial tests

4. **"No audio output"**
   - Check VoiceReel logs: `export VR_LOG_LEVEL=DEBUG`
   - Verify Fish-Speech models are downloaded correctly
   - Test with single speaker first

### Debug Mode

Enable detailed logging:
```bash
export VR_DEBUG=true
export VR_LOG_LEVEL=DEBUG
export VR_DEBUG_VERBOSE_LOGGING=true
```

View logs in real-time:
```bash
# API server logs
tail -f voicereel.log

# Celery worker logs
celery -A voicereel.tasks worker --loglevel=debug
```

## ğŸ“Š Performance Tips

1. **GPU Acceleration**
   - Ensure CUDA is available: `python -c "import torch; print(torch.cuda.is_available())"`
   - Use `export CUDA_VISIBLE_DEVICES=0` to specify GPU

2. **Batch Processing**
   - Process multiple dialogues in parallel
   - Use Redis for distributed task processing

3. **Audio Quality**
   - Use high-quality reference audio (44.1kHz, 16-bit)
   - Keep consistent recording conditions for speakers

## ğŸ¯ Next Steps

- Test with more speakers (3-5 person conversation)
- Try different languages (English, Japanese, Chinese)
- Experiment with emotional expressions
- Build a web UI for easier testing
- Deploy to production with HTTPS
